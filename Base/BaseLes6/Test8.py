"""
Чтение больших файлов

Списки Python

Эта функция открывает данный файл и использует file.read() вместе со .split() для того, чтобы добавить каждый ряд
данных как отдельный элемент списка. Если бы вы использовали эту версию cvs_reader() в блоке кода с подсчетом (вы его
увидите далее), тогда бы вы увидели следующее сообщение:

Traceback (most recent call last):
  File "ex1_naive.py", line 22, in <module>
    main()
  File "ex1_naive.py", line 13, in main
    csv_gen = csv_reader("file.txt")
  File "ex1_naive.py", line 6, in csv_reader
    result = file.read().split("\n")
MemoryError

В этом случае open() возвращает объект генератора, который вы можете «лениво» (не обсчитывая заранее) перебирать ряд за
рядом. Тем не менее, file.read().split() загружает все данные в память сразу, вызывая ошибку памяти (MemoryError).

До того как это произойдет, можно заметить, что ваш компьютер замедлился. Возможно потребуется даже вручную
остановить программу. Но что делать, если мы хотим этого избежать?

"""


def csv_reader(file_name):
    file = open(file_name)
    result = file.read().split("\n")
    return result


"""
Работа с потоками данных и большими файлами, такими например как CSV, являются наиболее распространенными вариантами
использования генераторов. Давайте возьмем CSV файл (CSV является стандартным форматом для обмена данными, колонки в
нем разделяются при помощи запятых). Предположим, что вы хотите посчитать количество имеющихся в нем рядов. Код ниже
предлагает один из путей для, того, чтобы осуществить это:
"""


csv_gen = csv_reader("../../../../Навчання/Base/Урок 6/examples/some_csv.txt")
row_count = 0
for row in csv_gen:
    row_count += 1
print(f"Row count is {row_count}")


"""
Глядя на этот пример, можно предположить что csv_gen является списком. Для того чтобы заполнить этот список, 
csv_reader() открывает файл и загружает его содержимое в csv_gen. Затем программа перебирает список, увеличивая 
значение row_count для каждого следующего ряда.

Это вполне приемлемое решение, но будет ли этот подход работать, если файл окажется слишком большим? А что если файл 
окажется больше чем вся доступная память, которая есть в нашем распоряжении? Для того чтобы ответить на этот вопрос, 
предположим, что csv_reader() будет открывать файл и считывать его в массив.
"""